{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other?\n",
    "    - Imagenette is smaller version of ImageNet, consisting 10 clases from the Imagenet which look very different to each other.\n",
    "    - For experiment, Imagenette will be better since it would require a quick and cheaper classifier, and those experiments works with ImageNet too.\n",
    "1. What is normalization?\n",
    "    - Normalization is to make the input 0 mean and standard deviation of 1\n",
    "1. Why didn't we have to care about normalization when using a pretrained model?\n",
    "    - We have to care about the normalizaion stats of the pretrained model since all the weights of the model is used to some preferences.\n",
    "    - fastai lib automatically adds the proper Normalize transform for the input so we didn't have to care about the normalization for `cnn_learner`\n",
    "1. What is progressive resizing?\n",
    "    - Start training using small images, and end training using large images.\n",
    "    - By spending most of the epochs with small images, training complete faster.\n",
    "    - By completing training using large image, the final accuracy will much higher.\n",
    "1. Implement progressive resizing in your own project. Did it help?\n",
    "1. What is test time augmentation? How do you use it in fastai?\n",
    "    - Select a number of areas to crop from the original rectangular image, pass each of them to the model, and take the max/avg of the predictions.\n",
    "1. Is using TTA at inference slower or faster than regular inference? Why?\n",
    "    - Slower since we have to validate the number of test time augmented images.\n",
    "1. What is Mixup? How do you use it in fastai?\n",
    "    - Mixup is a data augmentation technique, which can provide dramatically higher accuracy (esp when have small data, no pretrained model).\n",
    "    - Mixup works as follow for each image: \n",
    "        - Select another image from the dataset randomly\n",
    "        - Choose a scalar weight randomly\n",
    "        - Take a weighted average of the image with the chosen image ($w*x_{0} + (1-w)*x_{1}$); this will be independent variable\n",
    "        - Take a weighted average of the image's label with the chosen image's label ($w*y_{0} + (1-w)*y_{1}$); this will be dependent variable \n",
    "    - To use with fastai, we pass Mixup to the callback param (`cbs`) of Learner\n",
    "1. Why does Mixup prevent the model from being too confident?\n",
    "    - Since the pictures is mixup, the model has to predict two labels per image, rather than just one, as well as figuring out how much each one is weighted. Overfitting seems less likely to be a problem, because we're not showing the same image each epoch, but are instead showing a random combination of two images.\n",
    "1. Why does a training with Mixup for 5 epochs end up worse than a training without Mixup?\n",
    "    - Because mixup requires more epochs to train, since it is harder to train (hard to see what's in the picture)\n",
    "1. What is the idea behind label smoothing?\n",
    "    - Since in the classification problems, our target are one-hot encoded, which is 0-1. Even 0.999 is not good enough, the model will still try to get the gradients and learn to be more confident, which causing overfitting and gives you a model that is not going to give meaningful probabilities at inference time.\n",
    "    - It can become very harmful if your data is not perfectly labeled. In general, your data will never be perfect. Even if the labels were manually produced by humans, they could make mistakes, or have differences of opinions on images harder to label.\n",
    "    - Instead, we could replace all our `1`s by a number a bit less than `1`, and our `0`s by a number a bit more than `0`, and then train. This is called *label smoothing*. \n",
    "1. What problems in your data can label smoothing help with?\n",
    "    - By encouraging your model to be less confident, label smoothing will make your training more robust, even if there is mislabeled data, and will produce a model that generalizes better at inference.\n",
    "1. When using label smoothing with 5 categories, what is the target associated with the index 1?\n",
    "    - $\\frac{\\epsilon}{N}$ if original target is `0` or $1-\\epsilon + \\frac{\\epsilon}{N}$ otherwise\n",
    "1. What is the first step to take when you want to prototype quick experiments on a new dataset.\n",
    "    - Find a small subset is representative of the whole, then experiment on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
