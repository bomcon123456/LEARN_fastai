{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questionnaire"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is a greyscale image represented on a computer? How about a color image?\n",
    "\n",
    "    - A typical digital 2-D greyscale image is represented as a 2-D array (Width - Height) containing pixel value [0-255] (brightness)\n",
    "    - A 2-D color image is a greyscale image but has 3 stacks representing R,G,B\n",
    "1. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?\n",
    "    \n",
    "    - MNIST_SAMPLE consists of:\n",
    "        - train: Train set: a folder of \"3\" images and folder of \"7\" images\n",
    "        - validation: Validation set: same as train but for different purpose\n",
    "        - labels.csv: label for each image, 0 is 3 and 1 is 7\n",
    "1. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "    \n",
    "    - pixel simiarity means that we calculate how much closeness are there in the pictures, by comparing each pixel value by some function\n",
    "1. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "\n",
    "    - List comprehension is the pythonic way to do a for loop and result an array\n",
    "    ```python\n",
    "        arr = [i*i for i in range(10) if i%2!=0]\n",
    "    ```\n",
    "1. What is a \"rank 3 tensor\"?\n",
    "\n",
    "    - Rank 3 tensor is a tensor we must use 3 index so that we can access to a scalar\n",
    "    ```python\n",
    "        t3[0][0][0]\n",
    "    ```\n",
    "1. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "\n",
    "    - Rank is how many dimensions\n",
    "    - Shape is the length of each dimension\n",
    "    ```python\n",
    "        r = len(t.shape)\n",
    "    ```\n",
    "1. What are RMSE and L1 norm?\n",
    "    \n",
    "    - RMSE is Root mean square error: $RMSE = \\sqrt{\\frac{\\sum{(y_{1} - y_{0})^2}}{N}}$\n",
    "    - L1 norm is : $L1 = \\frac{\\sum^{N}{|y_{1} - y_{0}|}}{N}$\n",
    "1. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "\n",
    "    - Vectorization/ Broadcasting\n",
    "1. Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers.\n",
    "1. What is broadcasting?\n",
    "\n",
    "    - Broadcasting is a method for two tensors: one with smaller rank, but they can still operate. The rule specifies as follow:\n",
    "        - Compare element-wise each dimension from the trailing (last) dimension\n",
    "        - They are compatible only if:\n",
    "            - One of them is 1\n",
    "            - They're equal\n",
    "1. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "    \n",
    "    - Metrics should be calculated using the validation set since that is the purpose of the validations set. The model might have been overfitted the train set, which cause false accuracy if use.\n",
    "1. What is SGD?\n",
    "\n",
    "    - SGD (Stochastic Gradient Descent) is a method for optimizing a function so that we can find the global minimum if possible, but using a batch (rather than the whole dataset - which is Gradient Descent)\n",
    "1. Why does SGD use mini batches?\n",
    "\n",
    "    - Because sometime dataset is so big that if we use them all once would be very costly and take much time.\n",
    "1. What are the 7 steps in SGD for machine learning?\n",
    "    \n",
    "    - Initialize the weights\n",
    "    - For each observation, use the weights to predict\n",
    "    - Base of the prediction, calculate how good is the model\n",
    "    - Calculate the gradient (for each weight)\n",
    "    - Update all the weights with the gradient\n",
    "    - Go back to step 2, repeat\n",
    "    - Until the end of the training process\n",
    "1. How do we initialize the weights in a model?\n",
    "\n",
    "    - Naive thought: random values\n",
    "    - Good implementation: \n",
    "        - Xavier (for tanh): ![Xavier](https://miro.medium.com/max/1400/1*QIzXjH8uefVbcaycsjfdmw.png)\n",
    "        - Kaiming (a==0 for RELU): ![Kaiming](https://miro.medium.com/max/1032/0*DwUan_QhBFIKHFfy.png)\n",
    "1. What is \"loss\"?\n",
    "\n",
    "    - Loss is the function for us to see how well is our model, base on the difference between predictions made by the model and the targets\n",
    "1. Why can't we always use a high learning rate?\n",
    "    \n",
    "    - Because the gradient might be too big when multiply by the learning rate or simply the learning rate is too big in general, which might make us burst through the function to elsewhere (can't reach the minimum point) or pingpong around it.\n",
    "    ![lr1](./assets/lr_high_1.png)\n",
    "    ![lr2](./assets/lr_high_2.png)\n",
    "1. What is a \"gradient\"?\n",
    "\n",
    "    - The gradient tells us how much we have to change each weight to make our model better.\n",
    "1. Do you need to know how to calculate gradients yourself?\n",
    "\n",
    "    - Not necessary if we use framework\n",
    "1. Why can't we use accuracy as a loss function?\n",
    "\n",
    "    - Because accuracy doesn't drastically change when the model weights has small changes (which means gradients ~ 0), so the model can't learn much.\n",
    "1. Draw the sigmoid function. What is special about its shape?\n",
    "\n",
    "    ![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)\n",
    "1. What is the difference between loss and metric?\n",
    "    \n",
    "    - **accuracy** is a function that is constant almost everywhere (except at the threshold, 0.5) so its derivative is nil almost everywhere \n",
    "    - **loss function**: when our weights result in slightly better predictions, gives us a slightly better loss\n",
    "1. What is the function to calculate new weights using a learning rate?\n",
    "\n",
    "    $$w = w - lr*grad(w)$$\n",
    "1. What does the `DataLoader` class do?\n",
    "\n",
    "    - `DataLoader` will load the dataset with some configurations like batch_size, shuffle or not,... then we can iterate through the dataset effortlessly with it.\n",
    "1. Write pseudo-code showing the basic steps taken each epoch for SGD.\n",
    "\n",
    "    ```\n",
    "        for each epoch:\n",
    "            for x, y in dataloader:\n",
    "                preds = model(x)\n",
    "                loss = loss_func(preds, y)\n",
    "                grads = grad_func(loss)\n",
    "                w -= lr * grads\n",
    "    ```\n",
    "1. Create a function which, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?\n",
    "\n",
    "    ```python\n",
    "        def func(a1,a2):\n",
    "        return zip(a1, list(a2))\n",
    "    ```\n",
    "1. What does `view` do in PyTorch?\n",
    "    \n",
    "    - Return a new tensor with the shape specified, but still use the same memory block as the tensor call this function.\n",
    "1. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "    \n",
    "    - `bias` params are just as the name, bias for a specific neural network, so that not only the weights of the function decide everything about that activation.\n",
    "    - e.g: a movie not only \"good\" only because of its feature (length, type,...), but maybe the audience just hate that movie for some reason.\n",
    "1. What does the `@` operator do in python?\n",
    "    \n",
    "    - Matrix multiplication\n",
    "1. What does the `backward` method do?\n",
    "    \n",
    "    - Calculate the gradient of those variable that requires_grad() involving with the one calling the function.\n",
    "1. Why do we have to zero the gradients?\n",
    "    \n",
    "    - Because pytorch accumulate it.\n",
    "1. What information do we have to pass to `Learner`?\n",
    "\n",
    "    - dataloaders, model, optimize function, loss function, metric\n",
    "1. Show python or pseudo-code for the basic steps of a training loop.\n",
    "\n",
    "    ```\n",
    "        for each epoch:\n",
    "            for x, y in dataloader:\n",
    "                preds = model(x)\n",
    "                loss = loss_func(preds, y)\n",
    "                grads = grad_func(loss)\n",
    "                w -= lr * grads\n",
    "    ```    \n",
    "1. What is \"ReLU\"? Draw a plot of it for values from `-2` to `+2`.\n",
    "\n",
    "    - A nonlinear function: \n",
    "    $$RELU = max(0, x)$$\n",
    "1. What is an \"activation function\"?\n",
    "\n",
    "    - Activation function is mathematical equations that determine the output of a neural network \n",
    "1. What's the difference between `F.relu` and `nn.ReLU`?\n",
    "\n",
    "    - F.relu is a function for calculating\n",
    "    - nn.relu is a module (e.g: for using in nn.Sequential)\n",
    "1. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
    "\n",
    "    - Thanks to experiments with deep models by researchers, it shows that the model could perfom much better using many nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36],\n",
       "        [49, 64, 81]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = torch.arange(1,10).reshape((3,3))\n",
    "array.pow_(2)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create your own implementation of Learner from scratch, based on the training loop shown in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLearner:\n",
    "    def __init__(self, dataloader, model, opt_func, loss_func, metrics):\n",
    "        self.dl = dataloader\n",
    "        self.mdl = model\n",
    "        self.opt_func = opt_func\n",
    "        self.opt = None\n",
    "        self.loss_func = loss_func\n",
    "        if callable(metrics):\n",
    "            self.metrics = [metrics]\n",
    "        \n",
    "    def _calc_grad(self, x, y):\n",
    "        preds = self.mdl(x)\n",
    "        loss = self.loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        \n",
    "    def _train_epoch(self):\n",
    "        for x, y in self.dl.train:            \n",
    "            self._calc_grad(x, y)\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "    \n",
    "    def _validate_epoch(self):\n",
    "        res = {}\n",
    "        for metric in self.metrics:\n",
    "            accs = [metric(self.mdl(xb), yb) for xb, yb in self.dl.valid]\n",
    "            res[metric.__name__] = round(torch.stack(accs).mean().item(), 4)\n",
    "        return res\n",
    "    \n",
    "    def fit(self, epochs, lr):\n",
    "        self.opt = self.opt_func(self.mdl.parameters(), lr)\n",
    "        for i in range(epochs):\n",
    "            self._train_epoch()\n",
    "            print(self._validate_epoch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "learn = MLearner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_accuracy': 0.4932}\n",
      "{'batch_accuracy': 0.9101}\n",
      "{'batch_accuracy': 0.8145}\n",
      "{'batch_accuracy': 0.9067}\n",
      "{'batch_accuracy': 0.9316}\n",
      "{'batch_accuracy': 0.9438}\n",
      "{'batch_accuracy': 0.9555}\n",
      "{'batch_accuracy': 0.9624}\n",
      "{'batch_accuracy': 0.9648}\n",
      "{'batch_accuracy': 0.9668}\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
