{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU?\n",
    "\n",
    "    - Resize to large size (larger than intent size for learning) on CPU **first**: \n",
    "        - Purpose: \n",
    "            - Ensure all images has same size when copied to GPU\n",
    "            - Keep some spare space for us when we use data augmentation in GPU\n",
    "        - Why CPU?\n",
    "            - Copying batch of large size image CPU might take more time?\n",
    "            - Since in CPU we only resize, not doing so many other things, we can resize it in the CPU without lossing many time\n",
    "    - Resize to smaller size (for training) in GPU **later**:\n",
    "        - Purpose:\n",
    "            - Reach to the intent image size, with other data augmentation.\n",
    "        - Why GPU:\n",
    "            - Train in GPU\n",
    "            - GPU works faster for data augmentation because it is optimized for doing the same actions for a batch\n",
    "1. If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book website for suggestions.\n",
    "1. What are the two ways in which data is most commonly provided, for most deep learning datasets?\n",
    "    \n",
    "    - 1. Individual files representing items of data (text, image,...) organised into folder or file names representing its information\n",
    "    - 2. Table of data (e.g: csv), each row is an item (may include filenames providing a connection between the data in the table and other data)\n",
    "1. Look up the documentation for `L` and try using a few of the new methods is that it adds.\n",
    "1. Look up the documentation for the Python pathlib module and try using a few methods of the Path class.\n",
    "1. Give two examples of ways that image transformations can degrade the quality of the data.\n",
    "1. What method does fastai provide to view the data in a DataLoader?\n",
    "1. What method does fastai provide to help you debug a DataBlock?\n",
    "1. Should you hold off on training a model until you have thoroughly cleaned your data?\n",
    "1. What are the two pieces that are combined into cross entropy loss in PyTorch?\n",
    "1. What are the two properties of activations that softmax ensures? Why is this important?\n",
    "1. When might you want your activations to not have these two properties?\n",
    "1. Calculate the \"exp\" and \"softmax\" columns of <<bear_softmax>> yourself (i.e. in a spreadsheet, with a calculator, or in a notebook).\n",
    "1. Why can't we use torch.where to create a loss function for datasets where our label can have more than two categories?\n",
    "1. What is the value of log(-2)? Why?\n",
    "1. What are two good rules of thumb for picking a learning rate from the learning rate finder?\n",
    "1. What two steps does the fine_tune method do?\n",
    "1. In Jupyter notebook, how do you get the source code for a method or function?\n",
    "1. What are discriminative learning rates?\n",
    "1. How is a Python slice object interpreted when passed as a learning rate to fastai?\n",
    "1. Why is early stopping a poor choice when using one cycle training?\n",
    "1. What is the difference between resnet 50 and resnet101?\n",
    "1. What does to_fp16 do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
